{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/luyin/Desktop/project/Q&A.xlsx',header = 0)\n",
    "l = df['Breakout'].unique() # 79 unique analyst\n",
    "dic = {} #create dictionary for questions\n",
    "for category in l:\n",
    "    list_ = list(df.loc[df['Breakout']  == category]['Question'])\n",
    "    dic[category] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'FICC', 'going', 'dont', 'vote', 'to', 'hit', 'them', 'one', 'way', 'or', 'another', 'strong', 'dollar', 'did', 'seem', 'to', 'have', 'a', 'huge', 'impact', 'and', 'you', 'what', 'are', 'you', 'doing', 'I', 'm', 's', 'what', 'do', 'you', 'and', 'me', 'think', 'or', 'like', 'apples', 'and', 'Apple', 'is', 'looking', 'at', 'buying', 'and', 'bought', 'U', 'K', 'startup', 'for', 'billion', 'another', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "UNK_IDX = 0\n",
    "\n",
    "\n",
    "#tokenize sentence by sentence\n",
    "def question_split(input_):\n",
    "    list_ = []\n",
    "    for q in input_:\n",
    "        q = q.split('\\n')\n",
    "        if len(q) ==1:\n",
    "            list_.append(q)\n",
    "    return list_\n",
    "    \n",
    "def tokenize(sent):\n",
    "    sent = re.sub('y ou', 'you', sent)\n",
    "    sent = re.sub('y es', 'yes', sent)\n",
    "    sent = re.sub('v o', 'vo', sent)\n",
    "    sent = re.sub(\"don't\", 'dont', sent)\n",
    "    sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "#     sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #\n",
    "#     sent = re.sub('^[0-9]+', '', sent)\n",
    "    return sent.split()\n",
    "\n",
    "# tokens = tokenize(\" ? y ou FICC going dont v ote to hit them one way or another strong dollar did seem to have a huge impact and y ou, what are you doing? I'm 's what do you and me think or like apples and Apple is looking at buying and bought U.K. startup for $1 billion. '\\n' another sentence\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training and validation dataset\n",
    "dic_len = len(dic['Balance sheet'])\n",
    "all_data = dic['Balance sheet']\n",
    "train_data = dic['Balance sheet'][:500]\n",
    "val_data = dic['Balance sheet'][500: dic_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for q in dic['Balance sheet']:\n",
    "    tokens = tokenize(q)\n",
    "    all_tokens += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52578"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20 \n",
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    sequences = list()\n",
    "    for l in range(length, len(input_)):\n",
    "        seq = input_[l-length:l]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "#     print('Total Sequences: {}' .format(len(sequences)))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(dic, length):\n",
    "    out_list = []\n",
    "    raw = question_split(dic)\n",
    "    for i in range(len(raw)):\n",
    "        sent = tokenize(raw[i][0].lower())\n",
    "        q_sequence = build_sequence(length, sent)\n",
    "        out_list += q_sequence\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d = data_generator(all_data,20)\n",
    "train_d = data_generator(train_data,20)\n",
    "val_d = data_generator(val_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_d = data_generator(val_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on net interest income do you have an outlook for how the net interest income dollars could trend from here',\n",
       " 'net interest income do you have an outlook for how the net interest income dollars could trend from here assuming',\n",
       " 'interest income do you have an outlook for how the net interest income dollars could trend from here assuming that',\n",
       " 'income do you have an outlook for how the net interest income dollars could trend from here assuming that you',\n",
       " 'do you have an outlook for how the net interest income dollars could trend from here assuming that you don']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(all_tokens) #fit on texts\n",
    "# sequences = tokenizer.texts_to_sequences(data_input)\n",
    "# vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_input):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_tokens) #fit on texts\n",
    "    sequences = tokenizer.texts_to_sequences(data_input)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(sequences)):\n",
    "        X.append((sequences[i][:(SEQUENCE_LENGTH-1)]))\n",
    "        y.append((sequences[i][-1]))\n",
    "\n",
    "    y = array(y)\n",
    "    X = array(X)\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_label =  data_loader(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input, val_label =  data_loader(val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16037 16037 3078\n",
      "6028 6028 3078\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input), len(train_label), len(train_label[0]))\n",
    "print(len(val_input), len(val_label), len(val_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  90,  60, 121,  36,   2,  35,  84, 148,  22,  23,   1,  90,\n",
       "        60, 121, 679,  49, 479,  42])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "out_filename = 'Balance sheet.txt'\n",
    "save_file(all_d, out_filename)\n",
    "# load doc into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_file(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16037 16037 3078\n",
      "6028 6028 3078\n",
      "Train on 16037 samples, validate on 6028 samples\n",
      "Epoch 1/100\n",
      "16037/16037 [==============================] - 27s 2ms/step - loss: 6.5261 - acc: 0.0393 - val_loss: 6.0322 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.52612, saving model to LSTM_new1.hdf5\n",
      "Epoch 2/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.8869 - acc: 0.0512 - val_loss: 6.0525 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00002: loss improved from 6.52612 to 5.88693, saving model to LSTM_new1.hdf5\n",
      "Epoch 3/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.8662 - acc: 0.0512 - val_loss: 6.0789 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00003: loss improved from 5.88693 to 5.86618, saving model to LSTM_new1.hdf5\n",
      "Epoch 4/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.8542 - acc: 0.0512 - val_loss: 6.0797 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00004: loss improved from 5.86618 to 5.85423, saving model to LSTM_new1.hdf5\n",
      "Epoch 5/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.7726 - acc: 0.0512 - val_loss: 5.9708 - val_acc: 0.0509\n",
      "\n",
      "Epoch 00005: loss improved from 5.85423 to 5.77262, saving model to LSTM_new1.hdf5\n",
      "Epoch 6/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.6508 - acc: 0.0564 - val_loss: 5.9547 - val_acc: 0.0632\n",
      "\n",
      "Epoch 00006: loss improved from 5.77262 to 5.65078, saving model to LSTM_new1.hdf5\n",
      "Epoch 7/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 5.5433 - acc: 0.0643 - val_loss: 5.9438 - val_acc: 0.0702\n",
      "\n",
      "Epoch 00007: loss improved from 5.65078 to 5.54333, saving model to LSTM_new1.hdf5\n",
      "Epoch 8/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.4768 - acc: 0.0707 - val_loss: 5.9452 - val_acc: 0.0776\n",
      "\n",
      "Epoch 00008: loss improved from 5.54333 to 5.47685, saving model to LSTM_new1.hdf5\n",
      "Epoch 9/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.4240 - acc: 0.0761 - val_loss: 5.9451 - val_acc: 0.0692\n",
      "\n",
      "Epoch 00009: loss improved from 5.47685 to 5.42397, saving model to LSTM_new1.hdf5\n",
      "Epoch 10/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.3840 - acc: 0.0783 - val_loss: 5.9173 - val_acc: 0.0806\n",
      "\n",
      "Epoch 00010: loss improved from 5.42397 to 5.38403, saving model to LSTM_new1.hdf5\n",
      "Epoch 11/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 5.3365 - acc: 0.0869 - val_loss: 5.9126 - val_acc: 0.0907\n",
      "\n",
      "Epoch 00011: loss improved from 5.38403 to 5.33646, saving model to LSTM_new1.hdf5\n",
      "Epoch 12/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 5.2876 - acc: 0.0910 - val_loss: 5.8913 - val_acc: 0.0949\n",
      "\n",
      "Epoch 00012: loss improved from 5.33646 to 5.28758, saving model to LSTM_new1.hdf5\n",
      "Epoch 13/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 5.2244 - acc: 0.0969 - val_loss: 5.8812 - val_acc: 0.0941\n",
      "\n",
      "Epoch 00013: loss improved from 5.28758 to 5.22436, saving model to LSTM_new1.hdf5\n",
      "Epoch 14/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.1504 - acc: 0.1063 - val_loss: 5.8748 - val_acc: 0.0975\n",
      "\n",
      "Epoch 00014: loss improved from 5.22436 to 5.15037, saving model to LSTM_new1.hdf5\n",
      "Epoch 15/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 5.0825 - acc: 0.1141 - val_loss: 5.8454 - val_acc: 0.1030\n",
      "\n",
      "Epoch 00015: loss improved from 5.15037 to 5.08250, saving model to LSTM_new1.hdf5\n",
      "Epoch 16/100\n",
      "16037/16037 [==============================] - 21s 1ms/step - loss: 5.0193 - acc: 0.1218 - val_loss: 5.8511 - val_acc: 0.1067\n",
      "\n",
      "Epoch 00016: loss improved from 5.08250 to 5.01930, saving model to LSTM_new1.hdf5\n",
      "Epoch 17/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.9698 - acc: 0.1287 - val_loss: 5.8606 - val_acc: 0.1140\n",
      "\n",
      "Epoch 00017: loss improved from 5.01930 to 4.96976, saving model to LSTM_new1.hdf5\n",
      "Epoch 18/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.9194 - acc: 0.1386 - val_loss: 5.8581 - val_acc: 0.1160\n",
      "\n",
      "Epoch 00018: loss improved from 4.96976 to 4.91941, saving model to LSTM_new1.hdf5\n",
      "Epoch 19/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.8749 - acc: 0.1411 - val_loss: 5.8582 - val_acc: 0.1158\n",
      "\n",
      "Epoch 00019: loss improved from 4.91941 to 4.87492, saving model to LSTM_new1.hdf5\n",
      "Epoch 20/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.8364 - acc: 0.1455 - val_loss: 5.8548 - val_acc: 0.1181\n",
      "\n",
      "Epoch 00020: loss improved from 4.87492 to 4.83645, saving model to LSTM_new1.hdf5\n",
      "Epoch 21/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.8031 - acc: 0.1473 - val_loss: 5.9130 - val_acc: 0.1165\n",
      "\n",
      "Epoch 00021: loss improved from 4.83645 to 4.80306, saving model to LSTM_new1.hdf5\n",
      "Epoch 22/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.7704 - acc: 0.1495 - val_loss: 5.8666 - val_acc: 0.1199\n",
      "\n",
      "Epoch 00022: loss improved from 4.80306 to 4.77042, saving model to LSTM_new1.hdf5\n",
      "Epoch 23/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.7360 - acc: 0.1535 - val_loss: 5.8745 - val_acc: 0.1196\n",
      "\n",
      "Epoch 00023: loss improved from 4.77042 to 4.73598, saving model to LSTM_new1.hdf5\n",
      "Epoch 24/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.7030 - acc: 0.1510 - val_loss: 5.8893 - val_acc: 0.1211\n",
      "\n",
      "Epoch 00024: loss improved from 4.73598 to 4.70302, saving model to LSTM_new1.hdf5\n",
      "Epoch 25/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.6693 - acc: 0.1596 - val_loss: 5.8990 - val_acc: 0.1226\n",
      "\n",
      "Epoch 00025: loss improved from 4.70302 to 4.66926, saving model to LSTM_new1.hdf5\n",
      "Epoch 26/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.6362 - acc: 0.1638 - val_loss: 5.9210 - val_acc: 0.1254\n",
      "\n",
      "Epoch 00026: loss improved from 4.66926 to 4.63616, saving model to LSTM_new1.hdf5\n",
      "Epoch 27/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.6102 - acc: 0.1678 - val_loss: 5.9203 - val_acc: 0.1287\n",
      "\n",
      "Epoch 00027: loss improved from 4.63616 to 4.61020, saving model to LSTM_new1.hdf5\n",
      "Epoch 28/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.5676 - acc: 0.1703 - val_loss: 5.9105 - val_acc: 0.1325\n",
      "\n",
      "Epoch 00028: loss improved from 4.61020 to 4.56763, saving model to LSTM_new1.hdf5\n",
      "Epoch 29/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.5399 - acc: 0.1735 - val_loss: 5.9252 - val_acc: 0.1370\n",
      "\n",
      "Epoch 00029: loss improved from 4.56763 to 4.53991, saving model to LSTM_new1.hdf5\n",
      "Epoch 30/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.5060 - acc: 0.1794 - val_loss: 5.9277 - val_acc: 0.1374\n",
      "\n",
      "Epoch 00030: loss improved from 4.53991 to 4.50601, saving model to LSTM_new1.hdf5\n",
      "Epoch 31/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.4719 - acc: 0.1810 - val_loss: 5.9355 - val_acc: 0.1357\n",
      "\n",
      "Epoch 00031: loss improved from 4.50601 to 4.47188, saving model to LSTM_new1.hdf5\n",
      "Epoch 32/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.4423 - acc: 0.1858 - val_loss: 5.9287 - val_acc: 0.1398\n",
      "\n",
      "Epoch 00032: loss improved from 4.47188 to 4.44229, saving model to LSTM_new1.hdf5\n",
      "Epoch 33/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.4071 - acc: 0.1881 - val_loss: 5.9400 - val_acc: 0.1407\n",
      "\n",
      "Epoch 00033: loss improved from 4.44229 to 4.40714, saving model to LSTM_new1.hdf5\n",
      "Epoch 34/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.3740 - acc: 0.1910 - val_loss: 5.9626 - val_acc: 0.1387\n",
      "\n",
      "Epoch 00034: loss improved from 4.40714 to 4.37396, saving model to LSTM_new1.hdf5\n",
      "Epoch 35/100\n",
      "16037/16037 [==============================] - 24s 2ms/step - loss: 4.3440 - acc: 0.1946 - val_loss: 5.9473 - val_acc: 0.1430\n",
      "\n",
      "Epoch 00035: loss improved from 4.37396 to 4.34396, saving model to LSTM_new1.hdf5\n",
      "Epoch 36/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.3131 - acc: 0.1959 - val_loss: 5.9767 - val_acc: 0.1448\n",
      "\n",
      "Epoch 00036: loss improved from 4.34396 to 4.31307, saving model to LSTM_new1.hdf5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.2842 - acc: 0.1990 - val_loss: 5.9657 - val_acc: 0.1466\n",
      "\n",
      "Epoch 00037: loss improved from 4.31307 to 4.28415, saving model to LSTM_new1.hdf5\n",
      "Epoch 38/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.2531 - acc: 0.2019 - val_loss: 5.9779 - val_acc: 0.1462\n",
      "\n",
      "Epoch 00038: loss improved from 4.28415 to 4.25306, saving model to LSTM_new1.hdf5\n",
      "Epoch 39/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.2223 - acc: 0.2038 - val_loss: 5.9880 - val_acc: 0.1471\n",
      "\n",
      "Epoch 00039: loss improved from 4.25306 to 4.22233, saving model to LSTM_new1.hdf5\n",
      "Epoch 40/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.1986 - acc: 0.2052 - val_loss: 6.0106 - val_acc: 0.1452\n",
      "\n",
      "Epoch 00040: loss improved from 4.22233 to 4.19861, saving model to LSTM_new1.hdf5\n",
      "Epoch 41/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.1686 - acc: 0.2064 - val_loss: 6.0139 - val_acc: 0.1506\n",
      "\n",
      "Epoch 00041: loss improved from 4.19861 to 4.16856, saving model to LSTM_new1.hdf5\n",
      "Epoch 42/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.1387 - acc: 0.2105 - val_loss: 6.0294 - val_acc: 0.1501\n",
      "\n",
      "Epoch 00042: loss improved from 4.16856 to 4.13869, saving model to LSTM_new1.hdf5\n",
      "Epoch 43/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.1137 - acc: 0.2098 - val_loss: 6.0447 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00043: loss improved from 4.13869 to 4.11372, saving model to LSTM_new1.hdf5\n",
      "Epoch 44/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.0846 - acc: 0.2134 - val_loss: 6.0341 - val_acc: 0.1501\n",
      "\n",
      "Epoch 00044: loss improved from 4.11372 to 4.08460, saving model to LSTM_new1.hdf5\n",
      "Epoch 45/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 4.0554 - acc: 0.2144 - val_loss: 6.0478 - val_acc: 0.1508\n",
      "\n",
      "Epoch 00045: loss improved from 4.08460 to 4.05535, saving model to LSTM_new1.hdf5\n",
      "Epoch 46/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 4.0313 - acc: 0.2176 - val_loss: 6.0580 - val_acc: 0.1521\n",
      "\n",
      "Epoch 00046: loss improved from 4.05535 to 4.03130, saving model to LSTM_new1.hdf5\n",
      "Epoch 47/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 3.9998 - acc: 0.2173 - val_loss: 6.0752 - val_acc: 0.1535\n",
      "\n",
      "Epoch 00047: loss improved from 4.03130 to 3.99981, saving model to LSTM_new1.hdf5\n",
      "Epoch 48/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.9795 - acc: 0.2208 - val_loss: 6.0974 - val_acc: 0.1518\n",
      "\n",
      "Epoch 00048: loss improved from 3.99981 to 3.97949, saving model to LSTM_new1.hdf5\n",
      "Epoch 49/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.9518 - acc: 0.2226 - val_loss: 6.0966 - val_acc: 0.1505\n",
      "\n",
      "Epoch 00049: loss improved from 3.97949 to 3.95181, saving model to LSTM_new1.hdf5\n",
      "Epoch 50/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.9277 - acc: 0.2235 - val_loss: 6.1349 - val_acc: 0.1503\n",
      "\n",
      "Epoch 00050: loss improved from 3.95181 to 3.92767, saving model to LSTM_new1.hdf5\n",
      "Epoch 51/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.8951 - acc: 0.2256 - val_loss: 6.1300 - val_acc: 0.1544\n",
      "\n",
      "Epoch 00051: loss improved from 3.92767 to 3.89511, saving model to LSTM_new1.hdf5\n",
      "Epoch 52/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.8673 - acc: 0.2302 - val_loss: 6.1462 - val_acc: 0.1535\n",
      "\n",
      "Epoch 00052: loss improved from 3.89511 to 3.86733, saving model to LSTM_new1.hdf5\n",
      "Epoch 53/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 3.8409 - acc: 0.2323 - val_loss: 6.1646 - val_acc: 0.1491\n",
      "\n",
      "Epoch 00053: loss improved from 3.86733 to 3.84092, saving model to LSTM_new1.hdf5\n",
      "Epoch 54/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 3.8150 - acc: 0.2332 - val_loss: 6.1769 - val_acc: 0.1531\n",
      "\n",
      "Epoch 00054: loss improved from 3.84092 to 3.81503, saving model to LSTM_new1.hdf5\n",
      "Epoch 55/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.7908 - acc: 0.2359 - val_loss: 6.1943 - val_acc: 0.1533\n",
      "\n",
      "Epoch 00055: loss improved from 3.81503 to 3.79078, saving model to LSTM_new1.hdf5\n",
      "Epoch 56/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.7710 - acc: 0.2368 - val_loss: 6.2027 - val_acc: 0.1516\n",
      "\n",
      "Epoch 00056: loss improved from 3.79078 to 3.77096, saving model to LSTM_new1.hdf5\n",
      "Epoch 57/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.7382 - acc: 0.2408 - val_loss: 6.2275 - val_acc: 0.1520\n",
      "\n",
      "Epoch 00057: loss improved from 3.77096 to 3.73822, saving model to LSTM_new1.hdf5\n",
      "Epoch 58/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.7154 - acc: 0.2428 - val_loss: 6.2341 - val_acc: 0.1488\n",
      "\n",
      "Epoch 00058: loss improved from 3.73822 to 3.71542, saving model to LSTM_new1.hdf5\n",
      "Epoch 59/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.6906 - acc: 0.2449 - val_loss: 6.2455 - val_acc: 0.1528\n",
      "\n",
      "Epoch 00059: loss improved from 3.71542 to 3.69064, saving model to LSTM_new1.hdf5\n",
      "Epoch 60/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.6623 - acc: 0.2492 - val_loss: 6.2722 - val_acc: 0.1521\n",
      "\n",
      "Epoch 00060: loss improved from 3.69064 to 3.66230, saving model to LSTM_new1.hdf5\n",
      "Epoch 61/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.6386 - acc: 0.2507 - val_loss: 6.3033 - val_acc: 0.1505\n",
      "\n",
      "Epoch 00061: loss improved from 3.66230 to 3.63855, saving model to LSTM_new1.hdf5\n",
      "Epoch 62/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.6188 - acc: 0.2525 - val_loss: 6.3079 - val_acc: 0.1495\n",
      "\n",
      "Epoch 00062: loss improved from 3.63855 to 3.61878, saving model to LSTM_new1.hdf5\n",
      "Epoch 63/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.5894 - acc: 0.2555 - val_loss: 6.3449 - val_acc: 0.1498\n",
      "\n",
      "Epoch 00063: loss improved from 3.61878 to 3.58943, saving model to LSTM_new1.hdf5\n",
      "Epoch 64/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.5641 - acc: 0.2589 - val_loss: 6.3606 - val_acc: 0.1530\n",
      "\n",
      "Epoch 00064: loss improved from 3.58943 to 3.56409, saving model to LSTM_new1.hdf5\n",
      "Epoch 65/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.5436 - acc: 0.2598 - val_loss: 6.3855 - val_acc: 0.1483\n",
      "\n",
      "Epoch 00065: loss improved from 3.56409 to 3.54360, saving model to LSTM_new1.hdf5\n",
      "Epoch 66/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 3.5192 - acc: 0.2636 - val_loss: 6.3790 - val_acc: 0.1498\n",
      "\n",
      "Epoch 00066: loss improved from 3.54360 to 3.51915, saving model to LSTM_new1.hdf5\n",
      "Epoch 67/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.4954 - acc: 0.2669 - val_loss: 6.4106 - val_acc: 0.1466\n",
      "\n",
      "Epoch 00067: loss improved from 3.51915 to 3.49541, saving model to LSTM_new1.hdf5\n",
      "Epoch 68/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.4797 - acc: 0.2669 - val_loss: 6.4220 - val_acc: 0.1458\n",
      "\n",
      "Epoch 00068: loss improved from 3.49541 to 3.47972, saving model to LSTM_new1.hdf5\n",
      "Epoch 69/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.4513 - acc: 0.2704 - val_loss: 6.4492 - val_acc: 0.1486\n",
      "\n",
      "Epoch 00069: loss improved from 3.47972 to 3.45133, saving model to LSTM_new1.hdf5\n",
      "Epoch 70/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.4267 - acc: 0.2737 - val_loss: 6.4539 - val_acc: 0.1513\n",
      "\n",
      "Epoch 00070: loss improved from 3.45133 to 3.42672, saving model to LSTM_new1.hdf5\n",
      "Epoch 71/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.4008 - acc: 0.2769 - val_loss: 6.4780 - val_acc: 0.1539\n",
      "\n",
      "Epoch 00071: loss improved from 3.42672 to 3.40078, saving model to LSTM_new1.hdf5\n",
      "Epoch 72/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.3875 - acc: 0.2779 - val_loss: 6.4907 - val_acc: 0.1511\n",
      "\n",
      "Epoch 00072: loss improved from 3.40078 to 3.38753, saving model to LSTM_new1.hdf5\n",
      "Epoch 73/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.3584 - acc: 0.2846 - val_loss: 6.5121 - val_acc: 0.1520\n",
      "\n",
      "Epoch 00073: loss improved from 3.38753 to 3.35841, saving model to LSTM_new1.hdf5\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.3419 - acc: 0.2847 - val_loss: 6.5196 - val_acc: 0.1516\n",
      "\n",
      "Epoch 00074: loss improved from 3.35841 to 3.34186, saving model to LSTM_new1.hdf5\n",
      "Epoch 75/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.3194 - acc: 0.2852 - val_loss: 6.5249 - val_acc: 0.1521\n",
      "\n",
      "Epoch 00075: loss improved from 3.34186 to 3.31944, saving model to LSTM_new1.hdf5\n",
      "Epoch 76/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.2906 - acc: 0.2921 - val_loss: 6.5483 - val_acc: 0.1493\n",
      "\n",
      "Epoch 00076: loss improved from 3.31944 to 3.29058, saving model to LSTM_new1.hdf5\n",
      "Epoch 77/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.2683 - acc: 0.2921 - val_loss: 6.5676 - val_acc: 0.1491\n",
      "\n",
      "Epoch 00077: loss improved from 3.29058 to 3.26828, saving model to LSTM_new1.hdf5\n",
      "Epoch 78/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.2478 - acc: 0.2976 - val_loss: 6.5819 - val_acc: 0.1488\n",
      "\n",
      "Epoch 00078: loss improved from 3.26828 to 3.24783, saving model to LSTM_new1.hdf5\n",
      "Epoch 79/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.2260 - acc: 0.2972 - val_loss: 6.6244 - val_acc: 0.1448\n",
      "\n",
      "Epoch 00079: loss improved from 3.24783 to 3.22597, saving model to LSTM_new1.hdf5\n",
      "Epoch 80/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.2043 - acc: 0.3037 - val_loss: 6.6173 - val_acc: 0.1493\n",
      "\n",
      "Epoch 00080: loss improved from 3.22597 to 3.20428, saving model to LSTM_new1.hdf5\n",
      "Epoch 81/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.1851 - acc: 0.3045 - val_loss: 6.6702 - val_acc: 0.1425\n",
      "\n",
      "Epoch 00081: loss improved from 3.20428 to 3.18513, saving model to LSTM_new1.hdf5\n",
      "Epoch 82/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.1578 - acc: 0.3098 - val_loss: 6.6911 - val_acc: 0.1428\n",
      "\n",
      "Epoch 00082: loss improved from 3.18513 to 3.15777, saving model to LSTM_new1.hdf5\n",
      "Epoch 83/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.1419 - acc: 0.3111 - val_loss: 6.6791 - val_acc: 0.1468\n",
      "\n",
      "Epoch 00083: loss improved from 3.15777 to 3.14195, saving model to LSTM_new1.hdf5\n",
      "Epoch 84/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.1187 - acc: 0.3152 - val_loss: 6.7193 - val_acc: 0.1435\n",
      "\n",
      "Epoch 00084: loss improved from 3.14195 to 3.11866, saving model to LSTM_new1.hdf5\n",
      "Epoch 85/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.0986 - acc: 0.3229 - val_loss: 6.7185 - val_acc: 0.1443\n",
      "\n",
      "Epoch 00085: loss improved from 3.11866 to 3.09857, saving model to LSTM_new1.hdf5\n",
      "Epoch 86/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.0822 - acc: 0.3188 - val_loss: 6.7370 - val_acc: 0.1455\n",
      "\n",
      "Epoch 00086: loss improved from 3.09857 to 3.08219, saving model to LSTM_new1.hdf5\n",
      "Epoch 87/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 3.0526 - acc: 0.3271 - val_loss: 6.7606 - val_acc: 0.1462\n",
      "\n",
      "Epoch 00087: loss improved from 3.08219 to 3.05260, saving model to LSTM_new1.hdf5\n",
      "Epoch 88/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.0323 - acc: 0.3300 - val_loss: 6.7813 - val_acc: 0.1455\n",
      "\n",
      "Epoch 00088: loss improved from 3.05260 to 3.03227, saving model to LSTM_new1.hdf5\n",
      "Epoch 89/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 3.0053 - acc: 0.3330 - val_loss: 6.7922 - val_acc: 0.1440\n",
      "\n",
      "Epoch 00089: loss improved from 3.03227 to 3.00531, saving model to LSTM_new1.hdf5\n",
      "Epoch 90/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.9851 - acc: 0.3377 - val_loss: 6.8142 - val_acc: 0.1452\n",
      "\n",
      "Epoch 00090: loss improved from 3.00531 to 2.98511, saving model to LSTM_new1.hdf5\n",
      "Epoch 91/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 2.9672 - acc: 0.3375 - val_loss: 6.8426 - val_acc: 0.1423\n",
      "\n",
      "Epoch 00091: loss improved from 2.98511 to 2.96723, saving model to LSTM_new1.hdf5\n",
      "Epoch 92/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.9510 - acc: 0.3407 - val_loss: 6.8618 - val_acc: 0.1443\n",
      "\n",
      "Epoch 00092: loss improved from 2.96723 to 2.95101, saving model to LSTM_new1.hdf5\n",
      "Epoch 93/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.9249 - acc: 0.3448 - val_loss: 6.8717 - val_acc: 0.1425\n",
      "\n",
      "Epoch 00093: loss improved from 2.95101 to 2.92492, saving model to LSTM_new1.hdf5\n",
      "Epoch 94/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.8997 - acc: 0.3486 - val_loss: 6.8847 - val_acc: 0.1452\n",
      "\n",
      "Epoch 00094: loss improved from 2.92492 to 2.89973, saving model to LSTM_new1.hdf5\n",
      "Epoch 95/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.8788 - acc: 0.3524 - val_loss: 6.9274 - val_acc: 0.1466\n",
      "\n",
      "Epoch 00095: loss improved from 2.89973 to 2.87877, saving model to LSTM_new1.hdf5\n",
      "Epoch 96/100\n",
      "16037/16037 [==============================] - 24s 1ms/step - loss: 2.8566 - acc: 0.3568 - val_loss: 6.9359 - val_acc: 0.1432\n",
      "\n",
      "Epoch 00096: loss improved from 2.87877 to 2.85661, saving model to LSTM_new1.hdf5\n",
      "Epoch 97/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.8928 - acc: 0.3533 - val_loss: 6.9497 - val_acc: 0.1430\n",
      "\n",
      "Epoch 00097: loss did not improve from 2.85661\n",
      "Epoch 98/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.8365 - acc: 0.3582 - val_loss: 6.9596 - val_acc: 0.1463\n",
      "\n",
      "Epoch 00098: loss improved from 2.85661 to 2.83646, saving model to LSTM_new1.hdf5\n",
      "Epoch 99/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.8060 - acc: 0.3646 - val_loss: 6.9874 - val_acc: 0.1442\n",
      "\n",
      "Epoch 00099: loss improved from 2.83646 to 2.80603, saving model to LSTM_new1.hdf5\n",
      "Epoch 100/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.7795 - acc: 0.3717 - val_loss: 7.0121 - val_acc: 0.1453\n",
      "\n",
      "Epoch 00100: loss improved from 2.80603 to 2.77945, saving model to LSTM_new1.hdf5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-887c6c18fe45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_new1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# save the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer_new.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.__reduce__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.to_bytes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mto_bytes\u001b[0;34m(getters, exclude)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mserialized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bin_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/msgpack_numpy.py\u001b[0m in \u001b[0;36mpackb\u001b[0;34m(o, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPacker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(3078, 100, input_length= SEQUENCE_LENGTH-1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3078, activation='softmax'))\n",
    "\n",
    "#import the checkpoint to save current model\n",
    "filepath=\"LSTM_new1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "callbacks_list = [checkpoint, earlystopper]\n",
    "# compile model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the model\n",
    "print(len(train_input), len(train_label), len(train_label[0]))\n",
    "print(len(val_input), len(val_label), len(val_label[0]))\n",
    "model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "# categorical_crossentropy\n",
    " \n",
    "# save the model to filey\n",
    "model.save('model_new1.h5')\n",
    "# save the tokenizer\n",
    "plk.dump(tokenizer, open('tokenizer_new.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16037 samples, validate on 6028 samples\n",
      "Epoch 1/100\n",
      "16037/16037 [==============================] - 27s 2ms/step - loss: 2.8835 - acc: 0.3537 - val_loss: 7.0043 - val_acc: 0.1480\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.88352, saving model to LSTM_new1.hdf5\n",
      "Epoch 2/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.7484 - acc: 0.3751 - val_loss: 7.0576 - val_acc: 0.1422\n",
      "\n",
      "Epoch 00002: loss improved from 2.88352 to 2.74839, saving model to LSTM_new1.hdf5\n",
      "Epoch 3/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.7144 - acc: 0.3779 - val_loss: 7.0764 - val_acc: 0.1435\n",
      "\n",
      "Epoch 00003: loss improved from 2.74839 to 2.71435, saving model to LSTM_new1.hdf5\n",
      "Epoch 4/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.6924 - acc: 0.3831 - val_loss: 7.1094 - val_acc: 0.1447\n",
      "\n",
      "Epoch 00004: loss improved from 2.71435 to 2.69238, saving model to LSTM_new1.hdf5\n",
      "Epoch 5/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.6655 - acc: 0.3881 - val_loss: 7.1381 - val_acc: 0.1428\n",
      "\n",
      "Epoch 00005: loss improved from 2.69238 to 2.66552, saving model to LSTM_new1.hdf5\n",
      "Epoch 6/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.6463 - acc: 0.3958 - val_loss: 7.1369 - val_acc: 0.1425\n",
      "\n",
      "Epoch 00006: loss improved from 2.66552 to 2.64630, saving model to LSTM_new1.hdf5\n",
      "Epoch 7/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.6258 - acc: 0.3995 - val_loss: 7.1828 - val_acc: 0.1408\n",
      "\n",
      "Epoch 00007: loss improved from 2.64630 to 2.62578, saving model to LSTM_new1.hdf5\n",
      "Epoch 8/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.6009 - acc: 0.4024 - val_loss: 7.1890 - val_acc: 0.1438\n",
      "\n",
      "Epoch 00008: loss improved from 2.62578 to 2.60091, saving model to LSTM_new1.hdf5\n",
      "Epoch 9/100\n",
      "16037/16037 [==============================] - 22s 1ms/step - loss: 2.5847 - acc: 0.4048 - val_loss: 7.2233 - val_acc: 0.1413\n",
      "\n",
      "Epoch 00009: loss improved from 2.60091 to 2.58470, saving model to LSTM_new1.hdf5\n",
      "Epoch 10/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.5697 - acc: 0.4069 - val_loss: 7.2496 - val_acc: 0.1342\n",
      "\n",
      "Epoch 00010: loss improved from 2.58470 to 2.56969, saving model to LSTM_new1.hdf5\n",
      "Epoch 11/100\n",
      "16037/16037 [==============================] - 23s 1ms/step - loss: 2.5487 - acc: 0.4141 - val_loss: 7.2877 - val_acc: 0.1384\n",
      "\n",
      "Epoch 00011: loss improved from 2.56969 to 2.54870, saving model to LSTM_new1.hdf5\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb79d0cb70>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"LSTM_new1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "callbacks_list = [checkpoint, earlystopper]\n",
    "\n",
    "model.load_weights(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_input, train_label, validation_data =(val_input, val_label), batch_size= 200, epochs=100, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "plk.dump(tokenizer, open('tokenizer_new.pkl', 'wb'))\n",
    "model.save('model_new1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "might be a one year additional grant period if needed but just remind us of how much is private that\n",
      "\n",
      "you re putting on for the fed down how are you thinking about the industry are you seeing any signs of the industry will impact as you think that s a little bit more than construction and just isolating out that the optimal mix growth according to the balance sheet\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Balance sheet.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model_new1.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer_new.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "touch on your deposit base which is rather unique and can you elaborate on deposit costs and beta trends outside\n",
      "\n",
      "s basically been up i m wondering how much says how much of the balance sheet and then the consumer side and then it s due to the economy and the hedges will work and i m wondering how much more of the balance sheet and then you re thinking\n"
     ]
    }
   ],
   "source": [
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my question i just wanted to follow up so last year it looks like you grew your average earning assets\n",
      "\n",
      "in the commercial customers are you seeing any evidence of the fed moves how to the size of that portfolio and also the margin carry return and how you think about the fed funds sold of uncertainty in the past few years how are you thinking about the fed moves\n"
     ]
    }
   ],
   "source": [
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two percentage points q on q and up from year on year and i m just wondering how far do\n",
      "\n",
      "you think about the fed funds sold of uncertainty in the past few years in said it the fed funds sold of uncertainty for the fed funds rate risk and how much more banks are examples how to citigroup average balance sheet and how you re seeing a little bit\n"
     ]
    }
   ],
   "source": [
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
